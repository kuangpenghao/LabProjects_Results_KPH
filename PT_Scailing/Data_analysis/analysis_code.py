# -*- coding: utf-8 -*-
"""
è°ƒå‚ç»“æœåˆ†ææ¨¡æ¿
è¾“å…¥ï¼šè°ƒå‚ç»“æœ.xlsxï¼ˆåŒ…å«10ä¸ªå¸ƒå°”å¼€å…³åˆ— + eval/loss, eval/accuracyï¼‰
è¾“å‡ºï¼šç‰¹å¾é‡è¦æ€§ã€äº¤äº’æ•ˆåº”ã€æ¨èé…ç½®

æ–°å¢åŠŸèƒ½ï¼š
1. analyze_fixed_switches(df, switch_list, value_list, out_dir, loss_col)
   - å›ºå®šæŒ‡å®šå¼€å…³çš„å–å€¼ï¼Œåˆ†ææ»¡è¶³æ¡ä»¶çš„å®éªŒè®°å½•çš„ loss åˆ†å¸ƒæƒ…å†µ
   - ç”Ÿæˆç®±çº¿å›¾ã€å°æç´å›¾ã€ç›´æ–¹å›¾+KDEã€ECDF ç­‰å¯è§†åŒ–
   - è¾“å‡ºç»Ÿè®¡æ‘˜è¦ï¼ˆå‡å€¼ã€æ–¹å·®ã€æå€¼ç­‰ï¼‰ä¸º Excel æ–‡ä»¶

2. compare_fixed_switches(df, conditions_list, out_dir, loss_col)
   - å¯¹æ¯”å¤šç»„å›ºå®šå¼€å…³æ¡ä»¶ä¸‹çš„ loss åˆ†å¸ƒæƒ…å†µ
   - ç”Ÿæˆè”åˆçš„æ¯”è¾ƒå›¾è¡¨ï¼šå¹¶æ’ç®±çº¿å›¾/å°æç´å›¾ã€é‡å ç›´æ–¹å›¾+KDEã€ECDFå¯¹æ¯”ã€ç»Ÿè®¡é‡æŸ±çŠ¶å›¾
   - æ”¯æŒå¤šæ¡ä»¶åŒæ—¶æ¯”è¾ƒåˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import xgboost as xgb
import shap
import itertools
import os


def load_data(path="D:\_SHTU_\TKW_Lab\LabProjects_Results_KPH\PT_Scailing\Data_analysis\sweep.xlsx"):
    df = pd.read_excel(path)

    # è‡ªåŠ¨è¯†åˆ«10ä¸ªå¼€å…³åˆ—ï¼ˆæ ¹æ®ä½ æä¾›çš„åˆ—åï¼‰
    switch_cols = [
        'no_mask_diagonal',
        'no_vo_rope',
        'untie_attn_weights',
        'untie_ffn_weights',
        'untie_layerwise_weights',
        'use_ffn_after_attn',
        'use_gated_ffn',
        'use_rms_norm',
        'use_single_message_attn',
        'use_std_residual'
    ]

    missing = set(switch_cols) - set(df.columns)
    if missing:
        raise ValueError(f"ç¼ºå¤±å¼€å…³åˆ—: {missing}")

    df_switch = df[switch_cols].astype(int)
    y_loss = df['eval/loss']
    y_acc = df['eval/accuracy']

    print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡å®éªŒè®°å½•")
    return df, switch_cols, df_switch, y_loss, y_acc


def train_xgb_and_shap(X, y, n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42):
    print("\nğŸ¤– æ­£åœ¨è®­ç»ƒ XGBoost æ¨¡å‹å¹¶è®¡ç®— SHAP å€¼...")
    model = xgb.XGBRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        random_state=random_state
    )
    model.fit(X, y)

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    plt.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡ï¼ˆå¯é€‰ï¼‰
    plt.rcParams['axes.unicode_minus'] = False

    return model, explainer, shap_values


def plot_shap_summary(shap_values, X):
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X, plot_type="bar", show=False)
    plt.title("SHAP ç‰¹å¾é‡è¦æ€§ï¼ˆå¯¹ eval/loss çš„å½±å“ï¼‰")
    plt.tight_layout()
    plt.savefig("shap_importance.png", dpi=150)
    plt.show()

    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X, show=False)
    plt.title("SHAP æ±‡æ€»å›¾ï¼ˆçº¢=å¼€å…³å¼€ï¼Œè“=å¼€å…³å…³ï¼‰")
    plt.tight_layout()
    plt.savefig("shap_summary.png", dpi=150)
    plt.show()


def top5_recommendation(df, switch_cols):
    # è®¡ç®—å¹¶ä¿å­˜ Top-5 åˆ° Excelï¼ˆä¸ç›´æ¥è¾“å‡ºåˆ°ç»ˆç«¯ï¼‰
    top5 = df.nlargest(390, 'eval/accuracy')[switch_cols + ['eval/accuracy', 'eval/loss', 'Name']]
    #top5.to_excel("top5_recommendation.xlsx", index=False)

    # ä¿å­˜ Top-5 ä¸­å„å¼€å…³å¼€å¯æ¯”ä¾‹
    top5_switch_mean = top5[switch_cols].mean().sort_values(ascending=False)
    top5_switch_mean.to_frame(name='fraction_on').to_excel("top390_switch_fraction.xlsx")

    return top5


def compute_shap_interactions(explainer, X, switch_cols):
    print("\nğŸ”„ æ­£åœ¨è®¡ç®— SHAP äº¤äº’å€¼ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
    shap_interaction = explainer.shap_interaction_values(X)
    interaction_matrix = np.abs(shap_interaction).mean(axis=0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(interaction_matrix, xticklabels=switch_cols, yticklabels=switch_cols, cmap="Blues")
    plt.title("SHAP äº¤äº’å¼ºåº¦ï¼ˆç»å¯¹å€¼å‡å€¼ï¼‰")
    plt.tight_layout()
    plt.savefig("shap_interactions.png", dpi=150)
    plt.show()

    return shap_interaction, interaction_matrix


def pairwise_scan(df, X, shap_values, shap_interaction, switch_cols, out_dir="interaction_analysis"):
    print("\nğŸ” æ­£åœ¨æ‰§è¡Œå…¨å±€äº¤äº’æ‰«æï¼ˆæ¯å¼€å…³ vs å…¶ä½™9ä¸ªï¼‰...")
    os.makedirs(out_dir, exist_ok=True)

    interaction_summary = []

    for main_feat, other_feat in itertools.combinations(switch_cols, 2):
        group = df.groupby([main_feat, other_feat])['eval/loss'].agg(['mean', 'count'])
        if group.empty:
            continue

        best_combo = group.loc[group['mean'].idxmin()]
        best_val = best_combo['mean']
        best_count = int(best_combo['count'])
        best_config = group['mean'].idxmin()  # (switch1_val, switch2_val)

        try:
            idx1 = X.columns.get_loc(main_feat)
            idx2 = X.columns.get_loc(other_feat)
            interaction_strength = np.abs(shap_interaction[:, idx1, idx2]).mean()
        except Exception:
            interaction_strength = np.nan

        plt.figure(figsize=(6, 4))
        shap.dependence_plot(
            main_feat,
            shap_values,
            X,
            interaction_index=other_feat,
            show=False,
            x_jitter=0.3,
            alpha=0.7
        )
        plt.title(f"{main_feat} vs {other_feat}")
        plt.tight_layout()
        plt.savefig(f"{out_dir}/{main_feat}_vs_{other_feat}.png", dpi=120)
        plt.close()

        interaction_summary.append({
            'switch1': main_feat,
            'switch2': other_feat,
            'best_loss': best_val,
            'best_count': best_count,
            'switch1_val': best_config[0],
            'switch2_val': best_config[1],
            'interaction_strength': interaction_strength
        })

    summary_df = pd.DataFrame(interaction_summary)
    summary_df = summary_df.sort_values('best_loss')
    summary_df.to_excel("interaction_summary_pairs.xlsx", index=False)

    # ä¸åœ¨ç»ˆç«¯æ‰“å°è¡¨æ ¼ï¼Œç›´æ¥è¿”å› DataFrameï¼ˆæ–‡ä»¶å·²ä¿å­˜ï¼‰
    print(f"âœ… å…¨å±€äº¤äº’æ‰«æå®Œæˆï¼Œæ‘˜è¦å·²ä¿å­˜ä¸º interaction_summary_pairs.xlsxï¼ˆå…± {len(interaction_summary)} æ¡ï¼‰")
    return summary_df


def three_way_scan(df, X, shap_interaction, switch_cols, out_dir="interaction_analysis"):
    print("\nğŸ” æ­£åœ¨æ‰§è¡Œä¸‰å…ƒç»„åˆæ‰«æï¼ˆæ¯ 3 ä¸ªå¼€å…³ä¸€ç»„ï¼‰...")
    interaction_summary_three = []

    for trio in itertools.combinations(switch_cols, 3):
        a, b, c = trio
        group3 = df.groupby([a, b, c])['eval/loss'].agg(['mean', 'count'])
        if group3.empty:
            continue

        best_combo3 = group3.loc[group3['mean'].idxmin()]
        best_val3 = best_combo3['mean']
        best_count3 = int(best_combo3['count'])
        best_config3 = group3['mean'].idxmin()  # (a_val, b_val, c_val)

        try:
            i1 = X.columns.get_loc(a)
            i2 = X.columns.get_loc(b)
            i3 = X.columns.get_loc(c)
            pair12 = np.abs(shap_interaction[:, i1, i2]).mean()
            pair13 = np.abs(shap_interaction[:, i1, i3]).mean()
            pair23 = np.abs(shap_interaction[:, i2, i3]).mean()
            tri_interaction_strength = np.mean([pair12, pair13, pair23])
        except Exception:
            tri_interaction_strength = np.nan

        try:
            df_plot = group3.reset_index()
            df_plot['combo'] = df_plot.apply(lambda r: f"{int(r[a])}{int(r[b])}{int(r[c])}", axis=1)
            df_plot = df_plot.sort_values('mean')

            plt.figure(figsize=(8, 4))
            sns.barplot(data=df_plot, x='combo', y='mean', palette='viridis')
            plt.xlabel(f"ç»„åˆ({a},{b},{c}) è®¾ç½® (a b c)")
            plt.ylabel('mean eval/loss')
            plt.title(f"ä¸‰å…ƒç»„åˆ Loss: {a} {b} {c}")
            plt.tight_layout()
            plt.savefig(f"{out_dir}/three_{a}_{b}_{c}.png", dpi=140)
            plt.close()
        except Exception:
            pass

        interaction_summary_three.append({
            'switch1': a,
            'switch2': b,
            'switch3': c,
            'best_loss': best_val3,
            'best_count': best_count3,
            'switch1_val': best_config3[0],
            'switch2_val': best_config3[1],
            'switch3_val': best_config3[2],
            'interaction_strength': tri_interaction_strength
        })

    if interaction_summary_three:
        summary3_df = pd.DataFrame(interaction_summary_three)
        summary3_df = summary3_df.sort_values('best_loss')
        summary3_df.to_excel("interaction_summary_three.xlsx", index=False)
        print(f"âœ… ä¸‰å…ƒç»„åˆæ‰«æå®Œæˆï¼Œæ‘˜è¦å·²ä¿å­˜ä¸º interaction_summary_three.xlsxï¼ˆå…± {len(interaction_summary_three)} æ¡ï¼‰")
        return summary3_df
    else:
        print("âš ï¸ æœªç”Ÿæˆä»»ä½•ä¸‰å…ƒç»„åˆæ‘˜è¦ï¼ˆæ•°æ®å¯èƒ½ä¸è¶³ï¼‰")
        return pd.DataFrame()


def compute_three_way_model_interactions(model, X, switch_cols, sample_size=200, out_file="three_way_model_interactions.xlsx"):
    """
    è®¡ç®—åŸºäºæ¨¡å‹é¢„æµ‹çš„ä¸‰å…ƒäº¤äº’å¼ºåº¦ï¼ˆå¯¹æ¯ä¸ªä¸‰å…ƒç»„ï¼Œä½¿ç”¨æœ‰é™å·®åˆ†åœ¨æ ·æœ¬å­é›†ä¸Šè®¡ç®—ä¸‰é˜¶äº¤äº’é‡ï¼‰

    Returns a DataFrame with columns: switch1, switch2, switch3, model_interaction_strength
    """
    print("\nğŸ”¢ è®¡ç®—åŸºäºæ¨¡å‹çš„ä¸‰å…ƒäº¤äº’å¼ºåº¦ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
    n_samples = len(X)
    sample_size = min(sample_size, n_samples)
    rng = np.random.default_rng(42)

    results = []
    combos = list(itertools.product([0, 1], repeat=3))
    coeffs = np.array([(-1) ** (3 - sum(bits)) for bits in combos])

    # é¢„è½¬æ¢ X ä¸º numpy array ä»¥åŠ é€Ÿå¤åˆ¶å’Œä¿®æ”¹
    X_vals = X.values
    for trio in itertools.combinations(switch_cols, 3):
        a, b, c = trio
        i1 = X.columns.get_loc(a)
        i2 = X.columns.get_loc(b)
        i3 = X.columns.get_loc(c)

        # éšæœºæŠ½æ ·ç´¢å¼•
        if n_samples <= sample_size:
            sample_idx = np.arange(n_samples)
        else:
            sample_idx = rng.choice(n_samples, size=sample_size, replace=False)

        tri_effects = []
        try:
            for s_idx in sample_idx:
                base = X_vals[s_idx].reshape(1, -1)
                variants = np.repeat(base, 8, axis=0).copy()
                for k, bits in enumerate(combos):
                    variants[k, i1] = bits[0]
                    variants[k, i2] = bits[1]
                    variants[k, i3] = bits[2]

                preds = model.predict(variants)
                tri_val = np.sum(coeffs * preds) / 8.0
                tri_effects.append(tri_val)

            model_strength = float(np.mean(np.abs(tri_effects)))
        except Exception:
            model_strength = np.nan

        results.append({
            'switch1': a,
            'switch2': b,
            'switch3': c,
            'model_interaction_strength': model_strength
        })

    res_df = pd.DataFrame(results)
    if not res_df.empty:
        res_df = res_df.sort_values('model_interaction_strength', ascending=False)
        res_df.to_excel(out_file, index=False)
        print(f"âœ… ä¸‰å…ƒæ¨¡å‹äº¤äº’å¼ºåº¦è®¡ç®—å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º {out_file}ï¼ˆå…± {len(res_df)} æ¡ï¼‰")
    else:
        print("âš ï¸ æœªè®¡ç®—åˆ°ä»»ä½•ä¸‰å…ƒæ¨¡å‹äº¤äº’å¼ºåº¦ï¼ˆæ•°æ®å¯èƒ½ä¸è¶³ï¼‰")

    return res_df


def analyze_fixed_switches(df, switch_list, value_list, out_dir="fixed_switch_analysis", loss_col='eval/loss'):
    """
    ç»™å®šè‹¥å¹²å¼€å…³åå’Œå¯¹åº”çš„å–å€¼ï¼ˆ0/1ï¼‰ï¼Œè¿‡æ»¤å‡ºæ»¡è¶³è¿™äº›å›ºå®šå–å€¼çš„å®éªŒè®°å½•ï¼Œ
    å¹¶å¯¹è¿™äº›è®°å½•çš„ `loss_col` åˆ—åšåˆ†å¸ƒåˆ†æå’Œå¯è§†åŒ–ï¼šç®±çº¿å›¾ã€ç›´æ–¹å›¾+KDEã€å°æç´å›¾ã€ECDFï¼Œ
    åŒæ—¶è¾“å‡ºç»Ÿè®¡é‡ï¼ˆcount, mean, std, min, 25%, 50%, 75%, maxï¼‰ä¸º Excel æ–‡ä»¶ã€‚

    å‚æ•°:
    - df: åŸå§‹ DataFrameï¼ˆåŒ…å«å¼€å…³åˆ—ä¸ `loss_col`ï¼‰
    - switch_list: å¼€å…³åç§°åˆ—è¡¨ï¼Œä¾‹å¦‚ ['use_gated_ffn', 'use_rms_norm']
    - value_list: ä¸ä¹‹ç­‰é•¿çš„å–å€¼åˆ—è¡¨ï¼Œä¾‹å¦‚ [1, 0]
    - out_dir: è¾“å‡ºç›®å½•ï¼ˆå›¾ç‰‡ä¸ç»Ÿè®¡è¡¨ä¿å­˜åˆ°æ­¤å¤„ï¼‰
    - loss_col: è¦åˆ†æçš„æŸå¤±åˆ—åï¼ˆé»˜è®¤ 'eval/loss'ï¼‰

    è¿”å›:
    - stats_df: åŒ…å«ç»Ÿè®¡é‡çš„ DataFrame
    - subset_df: è¿‡æ»¤åçš„å­é›† DataFrame
    """
    import math

    os.makedirs(out_dir, exist_ok=True)

    # æ ¡éªŒè¾“å…¥
    if len(switch_list) != len(value_list):
        raise ValueError("switch_list ä¸ value_list å¿…é¡»ç­‰é•¿")

    for sw in switch_list:
        if sw not in df.columns:
            raise ValueError(f"å¼€å…³åˆ—ä¸å­˜åœ¨: {sw}")

    # ä¾æ¬¡è¿‡æ»¤
    mask = np.ones(len(df), dtype=bool)
    for sw, val in zip(switch_list, value_list):
        mask &= (df[sw] == int(val))

    subset = df[mask].copy()
    n = len(subset)
    if n == 0:
        print(f"âš ï¸ æ— ç¬¦åˆæ¡ä»¶çš„è®°å½•: {list(zip(switch_list, value_list))}")
        return pd.DataFrame(), subset

    # ç»Ÿè®¡é‡
    stats = subset[loss_col].describe()
    stats = stats.rename({"25%": "q1", "50%": "median", "75%": "q3"})
    stats_dict = {
        'count': int(stats['count']),
        'mean': float(stats['mean']),
        'std': float(stats['std']) if not np.isnan(stats['std']) else np.nan,
        'min': float(stats['min']),
        'q1': float(stats['q1']),
        'median': float(stats['median']),
        'q3': float(stats['q3']),
        'max': float(stats['max'])
    }
    stats_df = pd.DataFrame([stats_dict])
    stats_df['condition'] = ",".join([f"{s}={int(v)}" for s, v in zip(switch_list, value_list)])
    stats_df = stats_df.set_index('condition')
    stats_df.to_excel(os.path.join(out_dir, "fixed_switch_stats.xlsx"))

    # å‡ ç§å¯è§†åŒ–
    vals = subset[loss_col].dropna()

    # 1) ç®±çº¿å›¾ + å°æç´å›¾ï¼ˆå¹¶æ’ï¼‰
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    sns.boxplot(x=vals, color='skyblue')
    plt.title('ç®±çº¿å›¾')
    plt.xlabel(loss_col)

    plt.subplot(1, 2, 2)
    sns.violinplot(x=vals, color='lightgreen')
    plt.title('å°æç´å›¾')
    plt.xlabel(loss_col)
    plt.suptitle(f"å›ºå®š: {';'.join([f'{s}={int(v)}' for s,v in zip(switch_list,value_list)])} (n={n})")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(out_dir, "box_violin.png"), dpi=150)
    plt.close()

    # 2) ç›´æ–¹å›¾ + KDE
    plt.figure(figsize=(8, 5))
    sns.histplot(vals, kde=True, stat='density', color='cornflowerblue', bins=30)
    plt.axvline(stats_dict['mean'], color='red', linestyle='--', label=f"mean={stats_dict['mean']:.4f}")
    plt.axvline(stats_dict['median'], color='orange', linestyle='-.', label=f"median={stats_dict['median']:.4f}")
    plt.legend()
    plt.title('ç›´æ–¹å›¾ + KDE')
    plt.xlabel(loss_col)
    plt.ylabel('Density')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "hist_kde.png"), dpi=150)
    plt.close()

    # 3) å•ç‹¬çš„ KDE æ›²çº¿
    plt.figure(figsize=(8, 4))
    try:
        sns.kdeplot(vals, fill=True, color='purple')
        plt.title('æ¦‚ç‡å¯†åº¦ä¼°è®¡ (KDE)')
        plt.xlabel(loss_col)
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, "kde.png"), dpi=150)
    except Exception:
        # KDE å¯èƒ½åœ¨æ ·æœ¬å¤ªå°‘æ—¶å¤±è´¥ï¼Œé™çº§ä¸º histogram
        plt.clf()
        sns.histplot(vals, bins=30, color='gray')
        plt.title('Histogram (fallback for KDE)')
        plt.savefig(os.path.join(out_dir, "kde_fallback_hist.png"), dpi=150)
    plt.close()

    # 4) ECDF
    plt.figure(figsize=(8, 4))
    try:
        sns.ecdfplot(vals, color='teal')
        plt.title('ç»éªŒç´¯ç§¯åˆ†å¸ƒå‡½æ•° (ECDF)')
        plt.xlabel(loss_col)
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, "ecdf.png"), dpi=150)
    except Exception:
        pass
    plt.close()

    # ä¿å­˜å­é›†ä¸º csv ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æ
    subset.to_csv(os.path.join(out_dir, "filtered_subset.csv"), index=False)

    print(f"âœ… å·²å¯¹æ¡ä»¶ {list(zip(switch_list, value_list))} çš„ {n} æ¡è®°å½•å®Œæˆåˆ†æï¼Œè¾“å‡ºä¿å­˜åœ¨ {out_dir}/")
    return stats_df, subset


def compare_fixed_switches(df, conditions_list, out_dir="compare_fixed_switches", loss_col='eval/loss'):
    """
    å¯¹æ¯”å¤šç»„å›ºå®šå¼€å…³æ¡ä»¶ä¸‹çš„ loss åˆ†å¸ƒæƒ…å†µï¼Œç”Ÿæˆè”åˆçš„æ¯”è¾ƒå›¾è¡¨ã€‚
    
    å‚æ•°:
    - df: åŸå§‹ DataFrameï¼ˆåŒ…å«å¼€å…³åˆ—ä¸ `loss_col`ï¼‰
    - conditions_list: æ¡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ï¼š
        {
            'switch_list': ['å¼€å…³1', 'å¼€å…³2', ...],
            'value_list': [å–å€¼1, å–å€¼2, ...],
            'label': 'æ¡ä»¶æ ‡ç­¾'  # å¯é€‰ï¼Œç”¨äºå›¾ä¾‹æ˜¾ç¤º
        }
    - out_dir: è¾“å‡ºç›®å½•ï¼ˆå›¾ç‰‡ä¸ç»Ÿè®¡è¡¨ä¿å­˜åˆ°æ­¤å¤„ï¼‰
    - loss_col: è¦åˆ†æçš„æŸå¤±åˆ—åï¼ˆé»˜è®¤ 'eval/loss'ï¼‰
    
    è¿”å›:
    - combined_stats_df: åŒ…å«æ‰€æœ‰æ¡ä»¶ç»Ÿè®¡é‡çš„å¯¹æ¯” DataFrame
    - combined_data: åŒ…å«æ‰€æœ‰æ¡ä»¶æ•°æ®çš„ DataFrameï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰
    """
    os.makedirs(out_dir, exist_ok=True)
    
    all_stats = []
    all_data = []
    
    # ä¸ºæ¯ä¸ªæ¡ä»¶æå–æ•°æ®å’Œç»Ÿè®¡é‡
    for i, condition in enumerate(conditions_list):
        switch_list = condition['switch_list']
        value_list = condition['value_list']
        label = condition.get('label', f"æ¡ä»¶{i+1}: " + ",".join([f"{s}={v}" for s, v in zip(switch_list, value_list)]))
        
        # è¿‡æ»¤æ•°æ®
        mask = np.ones(len(df), dtype=bool)
        for sw, val in zip(switch_list, value_list):
            if sw not in df.columns:
                raise ValueError(f"å¼€å…³åˆ—ä¸å­˜åœ¨: {sw}")
            mask &= (df[sw] == int(val))
        
        subset = df[mask].copy()
        n = len(subset)
        
        if n == 0:
            print(f"âš ï¸ æ¡ä»¶ {label} æ— ç¬¦åˆæ¡ä»¶çš„è®°å½•")
            continue
            
        # ç»Ÿè®¡é‡
        vals = subset[loss_col].dropna()
        stats = vals.describe()
        stats_dict = {
            'condition': label,
            'count': int(stats['count']),
            'mean': float(stats['mean']),
            'std': float(stats['std']) if not np.isnan(stats['std']) else np.nan,
            'min': float(stats['min']),
            'q1': float(stats['25%']),
            'median': float(stats['50%']),
            'q3': float(stats['75%']),
            'max': float(stats['max'])
        }
        all_stats.append(stats_dict)
        
        # ä¸ºè”åˆåˆ†æå‡†å¤‡æ•°æ®
        subset_for_plot = subset[[loss_col]].copy()
        subset_for_plot['condition'] = label
        subset_for_plot['condition_idx'] = i
        all_data.append(subset_for_plot)
    
    if not all_stats:
        print("âš ï¸ æ²¡æœ‰ä»»ä½•æ¡ä»¶äº§ç”Ÿæœ‰æ•ˆæ•°æ®")
        return pd.DataFrame(), pd.DataFrame()
    
    # åˆå¹¶æ•°æ®
    combined_stats_df = pd.DataFrame(all_stats)
    combined_data = pd.concat(all_data, ignore_index=True)
    
    # ä¿å­˜ç»Ÿè®¡å¯¹æ¯”è¡¨
    combined_stats_df.to_excel(os.path.join(out_dir, "comparison_stats.xlsx"), index=False)
    
    # 1) å¹¶æ’ç®±çº¿å›¾æ¯”è¾ƒ
    plt.figure(figsize=(12, 6))
    sns.boxplot(data=combined_data, x='condition', y=loss_col, palette='Set2')
    plt.title('å¤šæ¡ä»¶ Loss åˆ†å¸ƒå¯¹æ¯” (ç®±çº¿å›¾)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "comparison_boxplot.png"), dpi=150)
    plt.close()
    
    # 2) å¹¶æ’å°æç´å›¾æ¯”è¾ƒ
    plt.figure(figsize=(12, 6))
    sns.violinplot(data=combined_data, x='condition', y=loss_col, palette='Set3')
    plt.title('å¤šæ¡ä»¶ Loss åˆ†å¸ƒå¯¹æ¯” (å°æç´å›¾)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "comparison_violin.png"), dpi=150)
    plt.close()
    
    # 3) é‡å çš„ç›´æ–¹å›¾+KDE
    plt.figure(figsize=(10, 6))
    colors = plt.cm.Set1(np.linspace(0, 1, len(conditions_list)))
    for i, (_, group) in enumerate(combined_data.groupby('condition')):
        vals = group[loss_col].dropna()
        plt.hist(vals, bins=30, alpha=0.6, label=group['condition'].iloc[0], 
                color=colors[i], density=True)
        try:
            sns.kdeplot(vals, color=colors[i], linewidth=2)
        except:
            pass
    plt.xlabel(loss_col)
    plt.ylabel('Density')
    plt.title('å¤šæ¡ä»¶ Loss åˆ†å¸ƒå¯¹æ¯” (é‡å ç›´æ–¹å›¾+KDE)')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "comparison_hist_kde.png"), dpi=150, bbox_inches='tight')
    plt.close()
    
    # 4) ECDF å¯¹æ¯”
    plt.figure(figsize=(10, 6))
    for i, (_, group) in enumerate(combined_data.groupby('condition')):
        vals = group[loss_col].dropna()
        try:
            sns.ecdfplot(vals, label=group['condition'].iloc[0], color=colors[i], linewidth=2)
        except:
            pass
    plt.xlabel(loss_col)
    plt.ylabel('ç´¯ç§¯æ¦‚ç‡')
    plt.title('å¤šæ¡ä»¶ Loss åˆ†å¸ƒå¯¹æ¯” (ECDF)')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "comparison_ecdf.png"), dpi=150, bbox_inches='tight')
    plt.close()
    
    # 5) ç»Ÿè®¡é‡å¯¹æ¯”æŸ±çŠ¶å›¾
    plt.figure(figsize=(15, 5))
    metrics = ['mean', 'std', 'median']
    for i, metric in enumerate(metrics, 1):
        plt.subplot(1, 3, i)
        plt.bar(range(len(all_stats)), [s[metric] for s in all_stats], color=colors[:len(all_stats)])
        plt.title(f'{metric.capitalize()} å¯¹æ¯”')
        plt.xticks(range(len(all_stats)), [s['condition'] for s in all_stats], rotation=45, ha='right')
        plt.ylabel(metric)
    plt.suptitle('å…³é”®ç»Ÿè®¡é‡å¯¹æ¯”')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "comparison_stats_bar.png"), dpi=150, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜åˆå¹¶æ•°æ®ä¸º CSV
    combined_data.to_csv(os.path.join(out_dir, "comparison_data.csv"), index=False)
    
    print(f"âœ… å·²å®Œæˆ {len(conditions_list)} ä¸ªæ¡ä»¶çš„å¯¹æ¯”åˆ†æï¼Œè¾“å‡ºä¿å­˜åœ¨ {out_dir}/")
    print("ğŸ“Š ç”Ÿæˆçš„å¯¹æ¯”å›¾è¡¨ï¼š")
    print("  - comparison_boxplot.png: ç®±çº¿å›¾å¯¹æ¯”")
    print("  - comparison_violin.png: å°æç´å›¾å¯¹æ¯”") 
    print("  - comparison_hist_kde.png: é‡å ç›´æ–¹å›¾+KDEå¯¹æ¯”")
    print("  - comparison_ecdf.png: ECDFå¯¹æ¯”")
    print("  - comparison_stats_bar.png: ç»Ÿè®¡é‡æŸ±çŠ¶å›¾å¯¹æ¯”")
    
    return combined_stats_df, combined_data


def main():
    df, switch_cols, df_switch, y_loss, y_acc = load_data()

    X = df_switch.copy()
    y = y_loss
    model, explainer, shap_values = train_xgb_and_shap(X, y)

    #plot_shap_summary(shap_values, X)
    #top5 = top5_recommendation(df, switch_cols)
    #shap_interaction, interaction_matrix = compute_shap_interactions(explainer, X, switch_cols)
    #three_model_df = compute_three_way_model_interactions(model, X, switch_cols, sample_size=200, out_file="three_way_model_interactions.xlsx")
    #summary2_df = pairwise_scan(df, X, shap_values, shap_interaction, switch_cols)
    #summary3_df = three_way_scan(df, X, shap_interaction, switch_cols)
    
    # å•ç‹¬åˆ†æç¤ºä¾‹
    #stats_df, subset_df = analyze_fixed_switches(df, ['untie_attn_weights', 'use_rms_norm'], [1, 0], out_dir="fixed_10")
    #stats_df, subset_df = analyze_fixed_switches(df, ['untie_attn_weights', 'use_rms_norm'], [1, 1], out_dir="fixed_11")
    
    # è”åˆå¯¹æ¯”åˆ†æç¤ºä¾‹
    '''
    conditions = [
        {
            'switch_list': ['untie_attn_weights', 'use_rms_norm'],
            'value_list': [1, 0],
            'label': 'untie_attn=1, rms_norm=0'
        },
        {
            'switch_list': ['untie_attn_weights', 'use_rms_norm'],
            'value_list': [1, 1],
            'label': 'untie_attn=1, rms_norm=1'
        }
    ]
    compare_stats_df, compare_data = compare_fixed_switches(df, conditions, out_dir="comparison_analysis_13")

    conditions = [
        {
            'switch_list': ['untie_attn_weights', 'use_rms_norm', 'use_std_residual'],
            'value_list': [1, 0, 1],
            'label': 'untie_attn=1, rms_norm=0, std_residual=1'
        },
        {
            'switch_list': ['untie_attn_weights', 'use_rms_norm'],
            'value_list': [1, 1, 1],
            'label': 'untie_attn=1, rms_norm=1, std_residual=1'
        }
    ]
    compare_stats_df, compare_data = compare_fixed_switches(df, conditions, out_dir="comparison_analysis_123")
    '''

    conditions = [
        {
            'switch_list': ['untie_layerwise_weights'],
            'value_list': [1],
            'label': 'untie_layerwise_weights=1'
        },
        {
            'switch_list': ['untie_layerwise_weights'],
            'value_list': [0],
            'label': 'untie_layerwise_weights=0'
        }
    ]
    compare_stats_df, compare_data = compare_fixed_switches(df, conditions, out_dir="comparison_analysis_5")

    


if __name__ == "__main__":
    main()


